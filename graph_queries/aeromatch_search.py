# -*- coding: utf-8 -*-
"""
This code opens a spark session and connects to the Neo4j Aeromatch database
to query the graph. It is set up to allow a user to query the database by 
specifying the names of source and destination airports as well as to generate 
data to evaluate the efficiency of the Pyspark breadth-first-search using 
different partition schemes.

Author: Pat McCornack
12/8/2022
"""

#%%
import pandas as pd
import pyspark
from pyspark.sql import SparkSession 
from graphframes import *
import time
import random

## Start Spark Session and read in Neo4j Data ----------------------------------
spark = SparkSession.builder.getOrCreate()

airports = spark.read.format("org.neo4j.spark.DataSource")\
 .option("url", "bolt://localhost:7687")\
 .option("authentication.basic.username", "neo4j")\
 .option("authentication.basic.password", "password")\
 .option("labels", "airport")\
 .load()
 
routes = spark.read.format("org.neo4j.spark.DataSource")\
  .option("url", "bolt://localhost:7687")\
  .option("authentication.basic.username", "neo4j")\
  .option("authentication.basic.password", "password")\
  .option("relationship", "route")\
  .option("relationship.nodes.map", "false")\
  .option("relationship.source.labels", "airport")\
  .option("relationship.target.labels", "airport")\
  .load()
 
airports.show()
routes.show()

#%%
## Prepare data for processing ------------------------------------------------

# Set number of partitions
num_partitions = 5
airports = airports.repartition(num_partitions)
print(airports.rdd.getNumPartitions())

routes = routes.repartition(num_partitions)
print(routes.rdd.getNumPartitions())


# Build the structure of the edges and vertices 
vertices = airports.withColumnRenamed("<id>", "id")\
            .select("id", "name", "city", "country", "airportID")
            
edges = routes.withColumnRenamed("<source.id>", "src")\
        .withColumnRenamed("<target.id>", "dst")\
        .withColumnRenamed("<routes.id>", "routes.id")\
        .select("src", "dst")
        
# vertices.show()
# edges.show()

# Create the GraphFrame to query 
airport_graph = GraphFrame(vertices, edges)
routes.cache()
airports.cache()


#%%
## Create random source/destination pairs to query

# src = []
# dst= []
# for i in range(0, 250):
#     src.append(random.randint(0, (vertices.count()-1)))
#     dst.append(random.randint(0, (vertices.count()-1)))

# pairs = pd.DataFrame(list(zip(src, dst)), columns = ['src', 'dst'])
# pairs.to_csv('G:/My Drive/WSU DA/CPTS 415 - Big Data/Project/aeromatch_code/pairs.csv', index = False)


## Read pairs generated by above in
pairs = pd.read_csv('G:/My Drive/WSU DA/CPTS 415 - Big Data/Project/aeromatch_code/graph_queries/pairs.csv')
src = pairs['src']
dst = pairs['dst']


# For processing using Neo4j script
# src_airport = []
# dst_airport = []

# for i in range(0, len(src) - 1):
#     src_airport.append(int(vertices.loc[vertices['id'] == src[i]].airportID))
#     dst_airport.append(int(vertices.loc[vertices['id'] == dst[i]].airportID))

# airportID_pairs = pd.DataFrame(list(zip(src_airport, dst_airport)), columns = ['src', 'dst'])
#airportID_pairs.to_csv('./airportID_pairs.csv', index = False)
    
#%%
## Search Function: Get path between airports ---------------------------------

# Take user input for airport names and then format for BFS
source = "Beijing Capital International Airport"
destination = "Hartsfield Jackson Atlanta International Airport"

source_formatted = "name = '" + source + "'"
destination_formatted = "name = '" + destination + "'"

# Run breadth-first-search
start_time = time.time()

paths = airport_graph.bfs(
    fromExpr = source_formatted, 
    toExpr = destination_formatted,
    maxPathLength = 5)
paths.show()

print("BFS: --- %s seconds ---" % (time.time() - start_time))

#%%
## Analytical Results: Get performance metrics in terms of time ---------------
times = []
hops = []
partitions = []

# For every pair of airports get the breadth-first-search runtime
for i in range(0, len(src)): 
    src_fmt = "id = '" + str(src[i]) + "'"
    dst_fmt = "id = '" + str(dst[i]) + "'"    
    
    # Time the bfs query
    start_time = time.time()
    paths = airport_graph.bfs(
        fromExpr = src_fmt, 
        toExpr = dst_fmt,
        maxPathLength = 5)
    times.append(time.time() - start_time)
    
    # Record # partitions for observation
    partitions.append(num_partitions)
    
    # Get length of path
    paths = paths.toPandas()
    if paths.shape[0] == 0:  # In the case there is no path
        hops.append(0)
    else:
        hops.append(int(paths.columns[-2][1]) + 1)

# Create dataframe and save data
names = ['src','dst','time','hops']
save_file = './run_data_' + str(num_partitions) + '.csv'
zipped = list(zip(src, dst, times, hops, partitions))
run_analysis = pd.DataFrame(zipped, columns = ['src','dst','time','hops', 'partitions'])

# run_analysis.to_csv(save_file, index = False)


#%%
## Metrics on data ------------------------------------------------------------
# The following returns how much memory each dataframe takes up

# Node data
airports.count()
airports_pd = airports.toPandas()
airports_pd.info()

# Relation data
routes.count()
routes_pd = routes.toPandas()
routes_pd.info()

# Partition size for nodes
for i, part in enumerate(airports.rdd.glom().collect()):
    airports_part = part

airports_part = pd.DataFrame(airports_part)
airports_part.info()

# Partition size for paths
for i, part in enumerate(routes.rdd.glom().collect()):
    routes_part = part

routes_part = pd.DataFrame(routes_part)
routes_part.info()
#%%